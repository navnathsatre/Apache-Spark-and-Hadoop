Sqoop Incremental last modified :

bash-4.2$ sqoop import --connect jdbc:mysql://cxln2.c.thelab-240901.internal/retail_db -username sqoopuser --password NHkkP876rp -table incre_lm -m 1 --check-column date_of_incr --merge-key id --incremental lastmodified --last-value 2019-01-01 --target-dir /user/achievesai8637/SaiMidhun/Sqoop_lmd


output:

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.6.2.0-205/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.6.2.0-205/accumulo/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
20/03/04 16:10:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.2.0-205
20/03/04 16:10:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
20/03/04 16:10:02 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
20/03/04 16:10:02 INFO tool.CodeGenTool: Beginning code generation
20/03/04 16:10:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `incre_lm` AS t LIMIT 1
20/03/04 16:10:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `incre_lm` AS t LIMIT 1
20/03/04 16:10:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/hdp/2.6.2.0-205/hadoop-mapreduce
Note: /tmp/sqoop-achievesai8637/compile/b687531c46b02072b0a7d1d342731c08/incre_lm.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
20/03/04 16:10:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-achievesai8637/compile/b687531c46b02072b0a7d1d342731c08/incre_lm.jar
20/03/04 16:10:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `incre_lm` AS t LIMIT 1
20/03/04 16:10:05 INFO tool.ImportTool: Incremental import based on column `date_of_incr`
20/03/04 16:10:05 INFO tool.ImportTool: Lower bound value: '2019-01-01'
20/03/04 16:10:05 INFO tool.ImportTool: Upper bound value: '2020-03-04 16:10:05.0'
20/03/04 16:10:05 WARN manager.MySQLManager: It looks like you are importing from mysql.
20/03/04 16:10:05 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
20/03/04 16:10:05 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
20/03/04 16:10:05 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
20/03/04 16:10:05 INFO mapreduce.ImportJobBase: Beginning import of incre_lm
20/03/04 16:10:05 INFO client.RMProxy: Connecting to ResourceManager at cxln2.c.thelab-240901.internal/10.142.1.2:8050
20/03/04 16:10:06 INFO client.AHSProxy: Connecting to Application History server at cxln2.c.thelab-240901.internal/10.142.1.2:10200
20/03/04 16:10:07 INFO db.DBInputFormat: Using read commited transaction isolation
20/03/04 16:10:07 INFO mapreduce.JobSubmitter: number of splits:1
20/03/04 16:10:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1582215230149_3460
20/03/04 16:10:08 INFO impl.YarnClientImpl: Submitted application application_1582215230149_3460
20/03/04 16:10:09 INFO mapreduce.Job: The url to track the job: http://cxln2.c.thelab-240901.internal:8088/proxy/application_1582215230149_3460/
20/03/04 16:10:09 INFO mapreduce.Job: Running job: job_1582215230149_3460
20/03/04 16:10:19 INFO mapreduce.Job: Job job_1582215230149_3460 running in uber mode : false
20/03/04 16:10:19 INFO mapreduce.Job:  map 0% reduce 0%
20/03/04 16:10:26 INFO mapreduce.Job:  map 100% reduce 0%
20/03/04 16:10:26 INFO mapreduce.Job: Job job_1582215230149_3460 completed successfully
20/03/04 16:10:26 INFO mapreduce.Job: Counters: 30
        File System Counters
                FILE: Number of bytes read=0
                FILE: Number of bytes written=169684
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=87
                HDFS: Number of bytes written=52
                HDFS: Number of read operations=4
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters 
                Launched map tasks=1
                Other local map tasks=1
                Total time spent by all maps in occupied slots (ms)=55008
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=4584
                Total vcore-milliseconds taken by all map tasks=4584
                Total megabyte-milliseconds taken by all map tasks=7041024
        Map-Reduce Framework
                Map input records=2
                Map output records=2
                Input split bytes=87
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=57
                CPU time spent (ms)=1530
                Physical memory (bytes) snapshot=257511424
                Virtual memory (bytes) snapshot=4613832704
                Total committed heap usage (bytes)=264765440
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=52
20/03/04 16:10:26 INFO mapreduce.ImportJobBase: Transferred 52 bytes in 21.0804 seconds (2.4667 bytes/sec)
20/03/04 16:10:26 INFO mapreduce.ImportJobBase: Retrieved 2 records.
20/03/04 16:10:26 INFO tool.ImportTool: Final destination exists, will run merge job.
20/03/04 16:10:27 INFO client.RMProxy: Connecting to ResourceManager at cxln2.c.thelab-240901.internal/10.142.1.2:8050
20/03/04 16:10:27 INFO client.AHSProxy: Connecting to Application History server at cxln2.c.thelab-240901.internal/10.142.1.2:10200
20/03/04 16:10:28 INFO input.FileInputFormat: Total input paths to process : 2
20/03/04 16:10:28 INFO mapreduce.JobSubmitter: number of splits:2
20/03/04 16:10:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1582215230149_3461
20/03/04 16:10:29 INFO impl.YarnClientImpl: Submitted application application_1582215230149_3461
20/03/04 16:10:29 INFO mapreduce.Job: The url to track the job: http://cxln2.c.thelab-240901.internal:8088/proxy/application_1582215230149_3461/
20/03/04 16:10:29 INFO mapreduce.Job: Running job: job_1582215230149_3461
20/03/04 16:10:40 INFO mapreduce.Job: Job job_1582215230149_3461 running in uber mode : false
20/03/04 16:10:40 INFO mapreduce.Job:  map 0% reduce 0%
20/03/04 16:10:49 INFO mapreduce.Job:  map 50% reduce 0%
20/03/04 16:10:50 INFO mapreduce.Job:  map 100% reduce 0%
20/03/04 16:10:55 INFO mapreduce.Job:  map 100% reduce 100%
20/03/04 16:10:55 INFO mapreduce.Job: Job job_1582215230149_3461 completed successfully
20/03/04 16:10:55 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=168
                FILE: Number of bytes written=508700
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=453
                HDFS: Number of bytes written=52
                HDFS: Number of read operations=9
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters 
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=166104
                Total time spent by all reduces in occupied slots (ms)=53328
                Total time spent by all map tasks (ms)=13842
                Total time spent by all reduce tasks (ms)=3333
                Total vcore-milliseconds taken by all map tasks=13842
                Total vcore-milliseconds taken by all reduce tasks=3333
                Total megabyte-milliseconds taken by all map tasks=21261312

                Map output records=4
                Map output bytes=154
                Map output materialized bytes=174
                Input split bytes=349
                Combine input records=0
                Combine output records=0
                Reduce input groups=2
                Reduce shuffle bytes=174
                Reduce input records=4
                Reduce output records=2
                Spilled Records=8
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=235
                CPU time spent (ms)=8110
                Physical memory (bytes) snapshot=2470354944
                Virtual memory (bytes) snapshot=16449998848
                Total committed heap usage (bytes)=2473590784
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=104
        File Output Format Counters 
                Bytes Written=52
20/03/04 16:10:56 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:
20/03/04 16:10:56 INFO tool.ImportTool:  --incremental lastmodified
20/03/04 16:10:56 INFO tool.ImportTool:   --check-column date_of_incr
20/03/04 16:10:56 INFO tool.ImportTool:   --last-value 2020-03-04 16:10:05.0
20/03/04 16:10:56 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')
-bash-4.2$ sqoop import --connect jdbc:mysql://cxln2.c.thelab-240901.internal/retail_db -username sqoopuser --password NHkkP876rp -table incre_lm -m 1 --check-column date_of_incr --merge-key id --incremental lastmodified --last-va
lue 2019-01-01 --target-dir /user/achievesai8637/SaiMidhun/Sqoop_lmd^C
-bash-4.2$ hadoop dfs -ls /user/achievesai8637/SaiMidhun/Sqoop_lmd/
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.
Found 2 items
-rw-r--r--   3 achievesai8637 achievesai8637          0 2020-03-04 16:10 /user/achievesai8637/SaiMidhun/Sqoop_lmd/_SUCCESS
-rw-r--r--   3 achievesai8637 achievesai8637         52 2020-03-04 16:10 /user/achievesai8637/SaiMidhun/Sqoop_lmd/part-r-00000
-bash-4.2$ hadoop dfs -cat  /user/achievesai8637/SaiMidhun/Sqoop_lmd/*
